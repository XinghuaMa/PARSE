PARSE 2022 challenge
==============================

The repository gives an example of how to process the PARSE challenge data (implementation based on 3D-Unet). Any other preprocessing is welcomed and any framework can be used for the challenge, the only requirement is to submit `nii.gz` files with result shapes consistent with the original CT images. This  repository also contains the code used to prepare the data of the challenge (data preprocessing, model training and submission result generation).

Requirements
------------
Python 3.6, PyTorch 1.6 and other common packages are listed in [`requirements.txt`](requirements.txt).

Organization
------------
Folders that aren't in the repository can be generated by running the corresponding function.

    ├── README.md
    ├── params.pkl                    <- Example parameters for 3D-Unet
    ├── requirements.txt              <- Requirements file of data processing
    ├── dataset
    │   ├── train                     <- Preprocessed training dataset folder
    │   │   ├── PA000005              <- training data (numpy array)
    │   │  ...   ├── dcm.npy
    │   │        └── label.npy
    │   ├── eval                      <- Preprocessing testing dataset folder
    │   │   ├── PA000013              <- testing data (numpy array)
    │   │  ...   ├── dcm.npy
    │   │        └── label.npy
    │   ├── dcm_volume_array.npy      <- Set of training data images
    │   └── label_volume_array.npy    <- Set of training data labels
    ├── submit
    │   ├── npy                       <- Prediction result numpy array folder
    │   │   ├── PA000013.npy
    │   │  ...
    │   └── nii                       <- Submission result folder
    │       ├── PA000013.nii.gz
    │      ...
    ├── exp
    │   ├── 0011_0011                 <- Training exp folder
    │  ...
    ├── feature.py                    <- Available functions
    ├── dataset.py                    <- Data preprocessing and loading
    ├── model.py                      <- 3D-Unet network model
    ├── losser.py                     <- Loss functions
    ├── train.py                      <- Model training
    ├── evalu.py                      <- Model evaluation and testing
    ├── submit.py                     <- Submit file generation
    └── config.py                     <- File paths and training parameters

Usage
------------
- The path of raw data (`root_raw_train_data` and `root_raw_eval_data`) needs to be set in [`config.py`](config.py).

- `dataset_preprocessing()` in [`dataset.py`](dataset.py) reads and preprocesses raw data for model training and testing.

- `training()` in [`train.py`](train.py) trains the model and saves the parameters, and training-related hyperparameters can be set in [`config.py`](config.py).

- After `root_model_param` in [`config.py`](config.py) setting, `submit_pred()` in [`submit.py`](submit.py) can read testing data and generate submission results.

Acknowledgements
------------
- https://github.com/wolny/pytorch-3dunet